# Call Centre Optimisation with MDP & Reinforcement Learning (PPO)

This project explores **optimising call centre operations** using two approaches:  
1. **Markov Decision Process (MDP) with Value Iteration**  
2. **Reinforcement Learning with Proximal Policy Optimisation (PPO)**  

The goal is to minimise **customer waiting cost**, reduce **queue length**, and improve **server utilisation** in a two-queue, two-server system. We compare a **model-based (Value Iteration)** method with a **model-free (PPO)** method to highlight their strengths and limitations.

---

## ğŸ“Œ Part A â€“ Value Iteration with MDP
- Models the call centre as a **Markov Decision Process (MDP)**  
- Uses **Bellmanâ€™s equation** in a value iteration framework  
- Optimises skill-based routing: assigning calls to servers based on expertise and service rate  
- Objective: **minimise waiting cost** over a finite horizon  
- Outcome: MDP-based routing reduced **queue length** and **waiting cost** significantly compared to FIFO methods  

---

## ğŸ“Œ Part B â€“ Reinforcement Learning with PPO
- Develops a **custom Gymnasium environment** for a call centre  
- Trains a **PPO agent** (via Stable-Baselines3) to minimise waiting costs  
- Compares RL results against Value Iteration baseline  
- Key metrics:  
  - âœ… Cumulative waiting cost  
  - âœ… Queue length  
  - âœ… Server utilisation  

**Findings:**  
- PPO â†’ More adaptive, stable, and effective in minimising waiting costs in dynamic environments  
- VI â†’ Performs well initially but degrades with increasing system complexity  

---

## ğŸ§® Assumptions & Definitions
- **System:** Two types of calls, two servers with different skills & service rates  
- **Arrival process:** Poisson distribution (finite horizon)  
- **Reward function:** Negative waiting cost  
- **Optimal Policy:** Assign calls dynamically to minimise customer waiting time  

---

## ğŸ“‚ Dataset
**Modified Call Centre Dataset** (two agents only)  
- Features:  
  - Agent  
  - Date / Time  
  - Topic  
  - Answered (Y/N)  
  - Resolved  
  - Speed of Answer (sec)  
  - Average Talk Duration  
  - Satisfaction Rating  

---

## ğŸ› ï¸ Tools & Technologies
- **Python**  
- Libraries:  
  - Data: `pandas`, `numpy`  
  - Simulation/Math: `scipy`, `scikit-learn`  
  - RL Environment: `gymnasium`, `stable-baselines3`  
  - Visualisation: `matplotlib`, `seaborn`  
- **Jupyter Notebook**  

---

## ğŸ“‚ Repository Structure
```
.
â”œâ”€â”€ ModifiedCallCenterTwo.xlsx              # Raw dataset
â”œâ”€â”€ Call_Centre_Optimisation_Part_A.ipynb   # MDP + Value Iteration
â”œâ”€â”€ Call_Centre_Optimisation_Part_B.ipynb   # PPO Reinforcement Learning
â”œâ”€â”€ requirements.txt                        # Dependencies
â””â”€â”€ README.md                               # Project documentation
```

---

## ğŸš€ Getting Started

### 1) Clone the repo
```bash
git clone https://github.com/tauhid6069/call-centre-optimisation.git
cd call-centre-optimisation
```

### 2) Install dependencies
```bash
pip install -r requirements.txt
```

### 3) Run the notebooks
```bash
jupyter notebook
```

---

## ğŸ“Š Results

- **Value Iteration (VI)**  
  - Low queue length in early stages  
  - Performance degraded over time (higher waiting cost and penalties)  

- **Proximal Policy Optimisation (PPO)**  
  - Consistent, stable performance  
  - Lower cumulative penalties  
  - Better server utilisation and reduced waiting times  

**Summary:**  
VI provides a strong mathematical baseline, but PPO adapts better to complex, uncertain environments.

---

## ğŸ”® Future Work
- Explore advanced PPO variants (e.g., **TR-PPO-RB**)  
- Extend to multi-skill, multi-server call centres  
- Test on **real-world call centre datasets**  
- Explore hybrid MDP + RL frameworks  

---

## ğŸ‘¤ Author
**Md Tauhidul Islam**  
[LinkedIn](https://www.linkedin.com/in/tauhidul-islam) 
